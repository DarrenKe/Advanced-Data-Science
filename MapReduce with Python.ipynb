{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1891bdcd87c6c960df75b6e90b02b234",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If you are not using the `Assignments` tab on the course JupyterHub server to read this notebook, read [Activating the assignments tab](https://github.com/lcdm-uiuc/info490-sp17/blob/master/help/act_assign_tab.md).\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says `YOUR CODE HERE`. Do **not** write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select _Kernel_, and restart the kernel and run all cells (_Restart & Run all_).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select _File_ → _Save and CheckPoint_)\n",
    "\n",
    "5. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "197039b9b8899dc1d4ed57d5a26aee17",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 12.2. MapReduce.\n",
    "\n",
    "In this problem, we will use Hadoop Streaming to execute a MapReduce code written in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f031162e2abc7fcf51f9dd275606ff76",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nose.tools import assert_equal, assert_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54f56796e10423a3f721bb427b1abd5c",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will use the [airline on-time performance data](http://stat-computing.org/dataexpo/2009/), but before proceeding, recall that the data set is encoded in `latin-1`. However, the Python 3 interpreter expects the standard input and output to be in `utf-8` encoding. Thus, we have to explicitly state that the Python interpreter should use `latin-1` for all IO operations, which we can do by setting the Python environment variable `PYTHONIOENCODING` equal to `latin-1`. We can set the environment variables of the current IPython kernel by modifying the `os.environ` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3e7906fde5d4b7f27e9927d261b3f089",
     "grade": false,
     "grade_id": "os_environ",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONIOENCODING'] = 'latin-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c65ffad078dda3747245ec34b0791730",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let's use the shell to check if the variable is set correctly. If you are not familiar with the following syntax (i.e., Python variable = ! shell command), [this notebook](https://github.com/UI-DataScience/info490-fa15/blob/master/Week4/assignment/unix_ipython.ipynb) from the previous semester might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8de6d2887a1792f2183e070174de9b84",
     "grade": false,
     "grade_id": "pythonioencoding",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "python_io_encoding = ! echo $PYTHONIOENCODING\n",
    "assert_equal(python_io_encoding.s, 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "56bc8908e5a07f21657ea08cc9d4ad20",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Mapper\n",
    "\n",
    "Write a Python script that\n",
    "  - Reads data from `STDIN`,\n",
    "  - Skips the first line (The first line of `2001.csv` is the header that has the column titles.)\n",
    "  - Outputs to `STDOUT` the `Origin` and `AirTime` columns separated with a tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5462e16b875b4b848820898cc6083cb3",
     "grade": false,
     "grade_id": "mapper_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# We explicitly define the word/count separator token.\n",
    "sep_in = ','\n",
    "sep_out = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "        \n",
    "        next(fin)\n",
    "        \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "        \n",
    "            # Strip off leading and trailing whitespace\n",
    "            line = line.strip(sep_in)\n",
    "#             print(line)\n",
    "#             break\n",
    "            \n",
    "            # We split the line into word tokens. Use whitespace to split.\n",
    "            # Note we don't deal with punctuation.\n",
    "            \n",
    "            words = line.split(sep_in)\n",
    "            Origin = words[13]\n",
    "            AirTime = words[16]\n",
    "\n",
    "            fout.write(\"{0}{1}{2}\\n\".format(AirTime, sep_out,Origin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "74b6b924b70d3777c7d46159dc6b3e8f",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We need make the file executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "45db6ad323b7869347842777aaadb992",
     "grade": false,
     "grade_id": "chmod_mapper",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! chmod u+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4438b3a411c0254c27b392517414a511",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Before testing the mapper code on the entire data set, let's first create a small file and test our code on this small data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "06147b916a7f5c463db26b61271f3166",
     "grade": false,
     "grade_id": "head_csv",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t60\n",
      "BWI\t64\n",
      "BWI\t80\n",
      "BWI\t66\n",
      "BWI\t62\n",
      "BWI\t61\n",
      "BWI\t61\n",
      "BWI\t60\n",
      "BWI\t52\n",
      "BWI\t62\n",
      "BWI\t62\n",
      "BWI\t55\n",
      "BWI\t60\n",
      "BWI\t61\n",
      "BWI\t63\n",
      "PHL\t53\n",
      "PHL\t54\n",
      "PHL\t55\n",
      "PHL\t53\n",
      "PHL\t50\n",
      "PHL\tNA\n",
      "PHL\t57\n",
      "PHL\t48\n",
      "PHL\t56\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t55\n",
      "PHL\t49\n",
      "PHL\t75\n",
      "PHL\t49\n",
      "PHL\t50\n",
      "PHL\t49\n",
      "PHL\tNA\n",
      "PHL\t46\n",
      "PHL\tNA\n",
      "PHL\t51\n",
      "PHL\t53\n",
      "PHL\t52\n",
      "PHL\t52\n",
      "PHL\t54\n",
      "PHL\t56\n",
      "PHL\t55\n",
      "PHL\t51\n",
      "PHL\t49\n",
      "PHL\t49\n",
      "CLT\t82\n",
      "CLT\t82\n",
      "CLT\t78\n"
     ]
    }
   ],
   "source": [
    "! head -n 50 $HOME/data/2001.csv > 2001.csv.head\n",
    "map_out_head = ! ./mapper.py < 2001.csv.head\n",
    "print('\\n'.join(map_out_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fa9dc67a4280d4e19f4aaae48d38554f",
     "grade": true,
     "grade_id": "mapper_test",
     "locked": true,
     "points": 6,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(\n",
    "    map_out_head,\n",
    "    ['BWI\\t60','BWI\\t64','BWI\\t80','BWI\\t66','BWI\\t62','BWI\\t61',\n",
    "     'BWI\\t61','BWI\\t60','BWI\\t52','BWI\\t62','BWI\\t62','BWI\\t55',\n",
    "     'BWI\\t60','BWI\\t61','BWI\\t63','PHL\\t53','PHL\\t54','PHL\\t55',\n",
    "     'PHL\\t53','PHL\\t50','PHL\\tNA','PHL\\t57','PHL\\t48','PHL\\t56',\n",
    "     'PHL\\t55','PHL\\t55','PHL\\t55','PHL\\t55','PHL\\t49','PHL\\t75',\n",
    "     'PHL\\t49','PHL\\t50','PHL\\t49','PHL\\tNA','PHL\\t46','PHL\\tNA',\n",
    "     'PHL\\t51','PHL\\t53','PHL\\t52','PHL\\t52','PHL\\t54','PHL\\t56',\n",
    "     'PHL\\t55','PHL\\t51','PHL\\t49','PHL\\t49','CLT\\t82','CLT\\t82',\n",
    "     'CLT\\t78']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a7b16d35034aff5d24ab9c626d05920f",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Reducer\n",
    "\n",
    "Write a Python script that\n",
    "\n",
    "  - Reads key-value pairs from `STDIN`,\n",
    "  - Computes the minimum and maximum air time for flights, with respect to each origin airport,\n",
    "  - Outputs to `STDOUT` the airports and the minimum and maximum air time for flights at each airport, separated with tabs.\n",
    "  \n",
    "For example,\n",
    "\n",
    "```shell\n",
    "$ ./mapper.py < 2001.csv.head | sort -n -k 1 | ./reducer.py\n",
    "```\n",
    "\n",
    "should give\n",
    "\n",
    "```\n",
    "BWI\t52\t80\n",
    "CLT\t78\t82\n",
    "PHL\t46\t75\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "577e94ce77f20e9fcab4548473e6068b",
     "grade": false,
     "grade_id": "reducer_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# We explicitly define the word/count separator token.\n",
    "sep = '\\t'\n",
    "\n",
    "# We open STDIN and STDOUT\n",
    "with sys.stdin as fin:\n",
    "    with sys.stdout as fout:\n",
    "    \n",
    "        # Keep track of current original, min and maxc\n",
    "        cOriginal = None\n",
    "        cMin = 0\n",
    "        cMax = 0\n",
    "        word = None\n",
    "   \n",
    "        # For every line in STDIN\n",
    "        for line in fin:\n",
    "\n",
    "            line = line.strip()            \n",
    "            \n",
    "            original, stime = line.split('\\t', 1)\n",
    "            \n",
    "            # We will assume count is always an integer value\n",
    "            if stime != 'NA':\n",
    "                stime = int(stime)\n",
    "\n",
    "                # original is either repeated or new\n",
    "\n",
    "                if cOriginal == original:\n",
    "                    if stime>cMax:\n",
    "                        cMax = stime\n",
    "                    if stime<cMin:\n",
    "                        cMin = stime\n",
    "\n",
    "                else:\n",
    "                    # We have to handle first word explicitly\n",
    "                    if cOriginal != None:\n",
    "                        fout.write(\"{0:s}{1:s}{2}{3:s}{4}\\n\".format(cOriginal, sep, cMin,sep,cMax))\n",
    "\n",
    "                    # New word, so reset variables\n",
    "                    cOriginal = original\n",
    "                    cMin = stime\n",
    "                    cMax = stime\n",
    "        else:\n",
    "            # Output final word count\n",
    "            if cOriginal == original:\n",
    "                fout.write(\"{0:s}{1:s}{2}{3:s}{4}\\n\".format(cOriginal, sep, cMin,sep,cMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "116d84eeb57b5821803b1c5dd6079cdd",
     "grade": false,
     "grade_id": "chmod_reducer",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! chmod u+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c32d9b3eb5c3cdd12d43453646c4a41e",
     "grade": false,
     "grade_id": "print_red_head_out",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWI\t52\t80\n",
      "CLT\t78\t82\n",
      "PHL\t46\t75\n"
     ]
    }
   ],
   "source": [
    "red_head_out = ! ./mapper.py < 2001.csv.head | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(red_head_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a4d522b73ff355b7ba4f081572853df1",
     "grade": true,
     "grade_id": "reducer_test",
     "locked": true,
     "points": 6,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(red_head_out, ['BWI\\t52\\t80','CLT\\t78\\t82','PHL\\t46\\t75'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "acacae226783a3ccc0f58e89b87f26c8",
     "grade": false,
     "grade_id": "markdown_7",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If the previous tests on the smaller data set were successful, we can run the mapreduce on the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1b626b777b93117328d274074ae30df2",
     "grade": false,
     "grade_id": "print_mapred_out",
     "locked": true,
     "points": 15,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABE\t16\t180\n",
      "ABI\t28\t85\n",
      "ABQ\t15\t264\n",
      "ACT\t19\t81\n",
      "ACY\t33\t33\n",
      "ADQ\t32\t67\n",
      "AKN\t12\t54\n",
      "ALB\t23\t360\n",
      "AMA\t30\t130\n",
      "ANC\t23\t428\n"
     ]
    }
   ],
   "source": [
    "mapred_out = ! ./mapper.py < $HOME/data/2001.csv | sort -n -k 1 | ./reducer.py\n",
    "print('\\n'.join(mapred_out[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "707e0440f71ecf50fd25df796055a7ed",
     "grade": true,
     "grade_id": "mapred_test",
     "locked": true,
     "points": 15,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(mapred_out), 231)\n",
    "assert_equal(mapred_out[:5], ['ABE\\t16\\t180', 'ABI\\t28\\t85', 'ABQ\\t15\\t264', 'ACT\\t19\\t81', 'ACY\\t33\\t33'])\n",
    "assert_equal(mapred_out[-5:], ['TYS\\t11\\t177', 'VPS\\t28\\t123', 'WRG\\t5\\t38', 'XNA\\t33\\t195', 'YAK\\t28\\t72'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a5494cd4c175f076e94595e7cf322f36",
     "grade": false,
     "grade_id": "markdown_8",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Reset\n",
    "\n",
    "We will do some cleaning up before we run Hadoop streaming. Let's first stop the [namenode and datanodes](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9d80b0ad0ee1800ef4bd250fb25d0834",
     "grade": false,
     "grade_id": "stop_dfs",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [info490rb.studentspace.cs.illinois.edu]\n",
      "info490rb.studentspace.cs.illinois.edu: no namenode to stop\n",
      "localhost: no datanode to stop\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: no secondarynamenode to stop\n",
      "stopping yarn daemons\n",
      "no resourcemanager to stop\n",
      "localhost: no nodemanager to stop\n",
      "no proxyserver to stop\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/sbin/stop-dfs.sh\n",
    "! $HADOOP_PREFIX/sbin/stop-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "58b6012aa82d530ffe36420eaf5e8b0d",
     "grade": false,
     "grade_id": "markdown_9",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "If there are any temporary files created during the previous Hadoop operation, we want to clean them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d6c4b22eb88a2b1b047ffd31c5f0dee7",
     "grade": false,
     "grade_id": "rm_tmp",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘/tmp/hsperfdata_root’: Operation not permitted\r\n"
     ]
    }
   ],
   "source": [
    "! rm -rf /tmp/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cb4cc641b35d74ac705e9e510076acd0",
     "grade": false,
     "grade_id": "markdown_10",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will simply [format the namenode](https://wiki.apache.org/hadoop/GettingStartedWithHadoop#Formatting_the_Namenode) and delete all files in our HDFS. Note that our HDFS is in an ephemeral Docker container, so all data will be lost anyway when the Docker container is shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f45a70c893a0b3bc36054ce691b8b77b",
     "grade": false,
     "grade_id": "format_namenode",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting using clusterid: CID-9dbe6ab9-527b-48f0-b594-dd6c6d9be2c5\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"Y\" | $HADOOP_PREFIX/bin/hdfs namenode -format 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e25ae4525c6108ddefb363a864f53810",
     "grade": false,
     "grade_id": "markdown_11",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "After formatting the namenode, we restart the namenode and datanodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d125ebad14694cb0db192ed38fa992b8",
     "grade": false,
     "grade_id": "start_dfs",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [info490rb.studentspace.cs.illinois.edu]\n",
      "info490rb.studentspace.cs.illinois.edu: starting namenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-namenode-info490rb.studentspace.cs.illinois.edu.out\n",
      "localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-datanode-info490rb.studentspace.cs.illinois.edu.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-data_scientist-secondarynamenode-info490rb.studentspace.cs.illinois.edu.out\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-info490rb.studentspace.cs.illinois.edu.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-data_scientist-nodemanager-info490rb.studentspace.cs.illinois.edu.out\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_PREFIX/etc/hadoop/hadoop-env.sh\n",
    "!$HADOOP_PREFIX/sbin/start-dfs.sh\n",
    "!$HADOOP_PREFIX/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d27f137ba10b75f650a95f8cdeda3359",
     "grade": false,
     "grade_id": "markdown_12",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sometimes when the namenode is restarted, it enteres Safe Mode, not allowing any changes to the file system. We do want to make changes, so we manually leave Safe Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "414927d448a62d71d8a59e3660a35a71",
     "grade": false,
     "grade_id": "leave_safemode",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is OFF\r\n"
     ]
    }
   ],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "52d35ba08290de85a816749e7d802358",
     "grade": false,
     "grade_id": "markdown_13",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Create directory\n",
    "\n",
    "- Create a new directory in HDFS at `/user/data_scientist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "87678b3b8b0718be86659d2c63226ef0",
     "grade": false,
     "grade_id": "mkdir_user_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a new directory in HDFS at /user/data_scientist.\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -rm -r -f /user\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/data_scientist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8ff7f2449db429a227393b7116d5429e",
     "grade": false,
     "grade_id": "print_user_dir",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2017-04-17 15:43 /user/data_scientist\n"
     ]
    }
   ],
   "source": [
    "ls_user = ! $HADOOP_PREFIX/bin/hdfs dfs -ls /user/\n",
    "print('\\n'.join(ls_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "66bd1e3aa00b79ffb656972b7aa35033",
     "grade": true,
     "grade_id": "mkdir_user_test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('/user/data_scientist' in ls_user.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "271d5fc842bc695a6a5e5f4c5ed1e3f4",
     "grade": false,
     "grade_id": "markdown_14",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Create a new directory in HDFS at `/user/data_scientist/wc/in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d942e42ca13f85183f1f3e60e4ab66e4",
     "grade": false,
     "grade_id": "mkdir_wc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a new directory in HDFS at `/user/data_scientist/wc/in`\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2bb267029f1cdcec08bd2729637d7f7c",
     "grade": false,
     "grade_id": "print_wc_1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - data_scientist supergroup          0 2017-04-17 15:45 wc/in\n"
     ]
    }
   ],
   "source": [
    "ls_wc = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
    "print('\\n'.join(ls_wc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3e2b6c77978895a2c0403c1e2fcc55ac",
     "grade": true,
     "grade_id": "mkdir_wc_test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/in' in ls_wc.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "13625f0e5ff95d5a949eec08a7553a6a",
     "grade": false,
     "grade_id": "markdown_15",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## HDFS: Copy\n",
    "\n",
    "- Copy `/home/data_scientist/data/2001.csv` from local file system into our new HDFS directory `wc/in`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "987b0deca0e75ca26456ff1b7b1a6468",
     "grade": false,
     "grade_id": "put_csv_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Copy `/home/data_scientist/data/2001.csv` from local file system into our new HDFS directory `wc/in`.\n",
    "\n",
    "!$HADOOP_PREFIX/bin/hdfs dfs -put /home/data_scientist/data/2001.csv /user/data_scientist/wc/in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1acd1a962a4241fe999e828e68e9041f",
     "grade": false,
     "grade_id": "print_wc_2",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 data_scientist supergroup  600411462 2017-04-17 15:48 wc/in/2001.csv\n"
     ]
    }
   ],
   "source": [
    "ls_wc_in = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc/in\n",
    "print('\\n'.join(ls_wc_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aade508e65d6b4ae41475d0e7fb5a4d0",
     "grade": true,
     "grade_id": "put_csv_test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/in/2001.csv' in ls_wc_in.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9e6cc3ff68930ecfa168bdda54d6a0d0",
     "grade": false,
     "grade_id": "markdown_16",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Python Hadoop Streaming\n",
    "\n",
    "- Run `mapper.py` and `reducer.py` via Hadoop Streaming.\n",
    "- Use `/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar`.\n",
    "- We need to pass the `PYTHONIOENCODING` environment variable to our Hadoop streaming task. To find out how to set `PYTHONIOENCODING` to `latin-1` in a Hadoop streaming task, use the `--help` and `-info` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a81465f086561001174b6537a2b423c4",
     "grade": false,
     "grade_id": "stream_answer",
     "locked": false,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar\n",
      "packageJobJar: [/tmp/hadoop-unjar7078479515767834151/] [] /tmp/streamjob4444813518648349044.jar tmpDir=null\n",
      "17/04/17 15:57:23 INFO client.RMProxy: Connecting to ResourceManager at info490rb.studentspace.cs.illinois.edu/192.168.4.5:8032\n",
      "17/04/17 15:57:23 INFO client.RMProxy: Connecting to ResourceManager at info490rb.studentspace.cs.illinois.edu/192.168.4.5:8032\n",
      "17/04/17 15:57:24 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "17/04/17 15:57:24 INFO mapreduce.JobSubmitter: number of splits:5\n",
      "17/04/17 15:57:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1492461811931_0001\n",
      "17/04/17 15:57:25 INFO impl.YarnClientImpl: Submitted application application_1492461811931_0001\n",
      "17/04/17 15:57:25 INFO mapreduce.Job: The url to track the job: http://info490rb.studentspace.cs.illinois.edu:8088/proxy/application_1492461811931_0001/\n",
      "17/04/17 15:57:25 INFO mapreduce.Job: Running job: job_1492461811931_0001\n"
     ]
    }
   ],
   "source": [
    "# Run Python code via Hadoop streaming\n",
    "\n",
    "# YOUR CODE HERE\n",
    "!ls $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming*\n",
    "# !$HADOOP_PREFIX/bin/hadoop --help\n",
    "# !%%bash\n",
    "# cd !$HOME/hadoop\n",
    "# Grab current streaming lib jar filename\n",
    "# !streaming_file=$(ls $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming*)\n",
    "\n",
    "# Run the Map Reduce task within Hadoop\n",
    "!$HADOOP_PREFIX/bin/hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar \\\n",
    "    -files mapper.py,reducer.py -input wc/in \\\n",
    "    -output wc/out -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d36e4eb945e5a26ee0d7d2a2e1f67416",
     "grade": false,
     "grade_id": "print_wc_out",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ls_wc_out = ! $HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
    "print('\\n'.join(ls_wc_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "df117688327be68ad0fbe76c8a1d0531",
     "grade": true,
     "grade_id": "stream_test_1",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_true('wc/out/_SUCCESS' in ls_wc_out.s)\n",
    "assert_true('wc/out/part-00000' in ls_wc_out.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "cc22509b73fa2029b07f12dd8c2c7f11",
     "grade": false,
     "grade_id": "print_wc_out_part",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stream_out = ! $HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000\n",
    "print('\\n'.join(stream_out[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c8bc6f9e83bafd7a8f559a7d995da5f0",
     "grade": true,
     "grade_id": "stream_test_2",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(mapred_out, stream_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "27c6c2b5c6026ed783833b4018ae4730",
     "grade": false,
     "grade_id": "markdown_17",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aaba4bec2c85276e5400a1e9929d286c",
     "grade": false,
     "grade_id": "cleanup",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! $HADOOP_PREFIX/bin/hdfs dfs -rm -r -f -skipTrash wc/out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
