{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c652e50c0f25904bd949daa398f86a74",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance, assert_is_not, assert_in\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we are going to analyze is a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. I randomly picked 5 constituents as the 5 attributes, and the `Class` column indicates types of wines. The dataset is taken from: https://archive.ics.uci.edu/ml/datasets/Wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8530e7904013464cb412779d04acc6b1",
     "grade": false,
     "grade_id": "data",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "wines = pd.read_csv('data/wines.csv', \n",
    "                    names=['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium',\n",
    "                           'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \n",
    "                           'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'],\n",
    "                    usecols=['Class', 'Alcohol', 'Ash', 'Alcalinity of ash', 'Total phenols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b97a639f282fc3358f751659992af702",
     "grade": false,
     "grade_id": "view",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol   Ash  Alcalinity of ash  Total phenols\n",
      "0      1    14.23  2.43               15.6           2.80\n",
      "1      1    13.20  2.14               11.2           2.65\n",
      "2      1    13.16  2.67               18.6           2.80\n",
      "3      1    14.37  2.50               16.8           3.85\n",
      "4      1    13.24  2.87               21.0           2.80\n"
     ]
    }
   ],
   "source": [
    "print(wines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "This function is the same function from [Problem 3.1](https://github.com/UI-DataScience/info490-sp17/blob/master/Week3/assignments/w3p1.ipynb). You can copy-paste your answer. You won't need to write this again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ae577066e805308ed991ce68c92c83eb",
     "grade": false,
     "grade_id": "split_function",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def split(df, test_column, test_size, random_state):\n",
    "    '''\n",
    "    Uses sklearn.train_test_split to split \"df\" into a testing set and a test set.\n",
    "    The \"test_columns\" lists the column that we are trying to predict.\n",
    "    All columns in \"df\" except \"test_columns\" will be used for training.\n",
    "    The \"test_size\" should be between 0.0 and 1.0 and represents the proportion of the\n",
    "    dataset to include in the test split.\n",
    "    The \"random_state\" parameter is used in sklearn.train_test_split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A pandas.DataFrame\n",
    "    test_columns: A list of strings\n",
    "    test_size: A float\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 4-tuple of pandas.DataFrames\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(test_column, axis=1).dropna(),df[test_column], test_size=test_size, random_state = random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `split` function, let's split our DataFrame 80:20 into training and tesing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "e4bf563206c66cbba851b393a229073b",
     "grade": false,
     "grade_id": "split_result",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(\n",
    "    df=wines,\n",
    "    test_column=['Class'],\n",
    "    test_size=0.2,\n",
    "    random_state=check_random_state(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier \n",
    "In the following code cell, write a function named `get_gbtc` that takes two hyperparameters: `n_estimators` and (optional) `max_depth` (Don't forget to include `random_state` as an argument as well), and returns a `sklearn.GradientBoostingClassifier` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "de33b06b6355df9c02227ae781894a45",
     "grade": false,
     "grade_id": "gbtc",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_gbtc(n_estimators, random_state, max_depth=1):\n",
    "    '''\n",
    "    A gradient boosting classifier with two adjustable hyperparameters:\n",
    "    \"n_estimators\" and (optional) \"max_depth\". \n",
    "    Don't forget to include \"random_state\".\n",
    "    Uses the default sklearn values for the remaining parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators: An int\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "    max_depth: An int, optional\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An sklearn.ensemble.GradientBoostingClassifier\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    gbtc = GradientBoostingClassifier(n_estimators=n_estimators, random_state=random_state, max_depth=max_depth)\n",
    "    \n",
    "    return gbtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7c5cd402ef48da85a664a9efd4e3a23e",
     "grade": true,
     "grade_id": "gbtc_assert",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t_gbtc = get_gbtc(200, check_random_state(2), max_depth=4)\n",
    "assert_is_instance(t_gbtc, ensemble.GradientBoostingClassifier)\n",
    "assert_equal(t_gbtc.n_estimators, 200)\n",
    "assert_equal(t_gbtc.max_depth, 4)\n",
    "assert_array_equal(t_gbtc.random_state.choice(100, 5), check_random_state(2).choice(100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and predict\n",
    "In the following code cell, write a function named `get_pred` that makes classification predictions on a testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "fc811cd2a1c3a87692ebc5a8aaa016cc",
     "grade": false,
     "grade_id": "pred",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_pred(gbtc, X_train, X_test, y_train):\n",
    "    '''\n",
    "    Fits the \"gbtc\" model on X_train and y_train.\n",
    "    Makes predictions on X_test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gbtc: A sklearn classifier instance.\n",
    "    X_train: A pandas.DataFrame\n",
    "    X_test: A pandas.DataFrame\n",
    "    y_train: A pandas.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    gbtc.fit(X_train, y_train)\n",
    "    y_pred = gbtc.predict(X_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c25f51468f563c1475808cacbb6790e7",
     "grade": true,
     "grade_id": "pred_assert",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t_gbtc = get_gbtc(200, check_random_state(1), max_depth=4)\n",
    "\n",
    "t_X_train = pd.DataFrame({\n",
    "    'X0': np.arange(100), \n",
    "    'X1': np.arange(-25, 75),\n",
    "    })\n",
    "\n",
    "t_y_train = pd.DataFrame({\n",
    "    'y': list(np.arange(10))*10\n",
    "    })\n",
    "\n",
    "t_X_test = pd.DataFrame({\n",
    "    'X0': np.arange(100),\n",
    "    'X1': np.arange(100),\n",
    "    })\n",
    "\n",
    "test_pred = get_pred(t_gbtc, t_X_train, t_X_test, t_y_train)\n",
    "assert_array_equal(test_pred, \n",
    "                   np.array([0, 1, 7, 3, 4, 0, 1, 2, 0, 4, 0, 1, 2, 3, 4, 0, 1, 2, 8, 4, 5, 1, 2,\n",
    "                             8, 4, 5, 1, 7, 1, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 2, 5, 6, 7, 3, 4, 5,\n",
    "                             6, 7, 3, 4, 5, 6, 7, 3, 4, 5, 6, 7, 8, 9, 0, 6, 7, 3, 4, 0, 1, 7, 3,\n",
    "                             4, 5, 1, 7, 8, 4, 5, 6, 7, 8, 9, 0, 1, 9, 3, 9, 5, 6, 7, 8, 9, 0, 1,\n",
    "                             9, 9, 4, 5, 6, 7, 8, 9]))\n",
    "\n",
    "y_pred = get_pred(t_gbtc, X_train, X_test, y_train)\n",
    "assert_array_equal(y_pred, np.array([1, 3, 2, 1, 1, 2, 1, 2, 2, 2, 3, 3, 1, 2, 3, 2, 1, 1, 2, 1, 1, 1, 1,\n",
    "                                     2, 2, 2, 2, 2, 2, 3, 1, 1, 2, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best hyperparameter\n",
    "Use [`sklearn.grid_search.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to write a function named `get_best_param` that takes `X_train`, `y_train`, `n_estimators`, `random_state`, and a range of possible `max_depth`. The function does a grid search on `max_depth`, and returns a `max_depth` value that optimizes the predicition result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "df71c71d6315a49c7c584394cd60c5d0",
     "grade": false,
     "grade_id": "best_param",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_best_param(X_train, y_train, n_estimators, random_state, max_depth_range):\n",
    "    '''\n",
    "    Implements a grid search on \"max_depth\" and returns the best \"max_depth\" value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A pandas.DataFrame\n",
    "    y_train: A pandas.DataFrame\n",
    "    n_estimators: An int\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "    max_depth_range: A numpy array of integers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An int\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    gbtc = GradientBoostingClassifier(n_estimators=n_estimators, random_state=random_state, max_depth=max_depth_range[0])\n",
    "    param_grid = {'max_depth':max_depth_range}\n",
    "    best_max_depth = GridSearchCV(estimator=gbtc, param_grid =param_grid)\n",
    "    best_max_depth.fit(X_train, np.ravel(y_train))\n",
    "    best_max_depth = best_max_depth.best_params_['max_depth']\n",
    "    \n",
    "    return best_max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3cc1d447652a1dda63777d17fa2d9a66",
     "grade": true,
     "grade_id": "best_param_assert",
     "locked": true,
     "points": 15,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This may take some time\n",
    "t_best_max_depth = get_best_param(X_train, y_train, 500, check_random_state(0), np.arange(1, 7))\n",
    "assert_equal(t_best_max_depth, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the effect of boosting\n",
    "Write a function named `plot_mean_abs_error` that plots [`mean_absolute_error`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) as a function of number of estimators for both training data and testing data (two curves). You should first reconstruct the Gradient Boosting Classifier model with `best_max_depth`, and fit and make predictions again. To avoid repeating code, you could take advantage of previously defined functions. You'll need to calculate the mean absolute error for each individual estimator. To access individual estimators, you'll find  [`staged_predict`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier.staged_predict) very helpful.\n",
    "\n",
    "To pass the assertion test, **you must label your curves as \"`Train`\" and \"`Test`\"** as indicated in this example image: \n",
    "![](./images/w4p3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "56139b77095ebf6acf0328bcb73a28f9",
     "grade": false,
     "grade_id": "plot",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_mean_abs_error(X_train, X_test, y_train, y_test, n_estimators, random_state, best_max_depth):\n",
    "    '''\n",
    "    Using the best parameter we found, reconstructs a gradient boosting classifier.\n",
    "    Fits the classifier and makes predictions.\n",
    "    Gets mean absolute error for all individual estimators, and makes a plot\n",
    "    of mean absolute value (on y-axis) vs. number of estimators (on x-axis)\n",
    "    for both training data and testing data.\n",
    "    Label your two curves as \"Test\" and \"Train\" (exact), respectively. \n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    X_train: A pandas.DataFrame\n",
    "    X_test: A pandas.DataFrame\n",
    "    y_train: A pandas.DataFrame\n",
    "    y_test: A pandas.DataFrame\n",
    "    n_estimators: An int\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "    best_max_depth: An int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A matplotlib.Axes instance\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    gbtc = get_gbtc(n_estimators, random_state, max_depth=best_max_depth)\n",
    "    gbtc.fit(X_train, y_train)\n",
    "    \n",
    "    n_est = len(gbtc.estimators_)\n",
    "    dev_test = np.zeros(n_est)\n",
    "    dev_train = np.zeros(n_est)\n",
    "    for idx, y_pred in enumerate(gbtc.staged_predict(X_test)):\n",
    "        dev_test[idx] = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    for idx, y_pred in enumerate(gbtc.staged_predict(X_train)):\n",
    "        dev_train[idx] = mean_absolute_error(y_train, y_pred)\n",
    "        \n",
    "    sns.set(style=\"white\")\n",
    "    sns.set(style=\"ticks\", font_scale=2.0)\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    ax.plot(np.arange(n_est) + 1, dev_test, color=sns.xkcd_rgb[\"denim blue\"], label='Test', lw=3)\n",
    "    ax.plot(np.arange(n_est) + 1, dev_train, color=sns.xkcd_rgb[\"pale red\"], label='Train', lw=3)\n",
    "    ax.set_title(\"Mean Absolute Error vs # Estimators\")\n",
    "    ax.set_ylabel('Mean Absolute Error')\n",
    "    ax.set_xlabel('# Estimators')\n",
    "\n",
    "    sns.despine(offset=0, trim=True)\n",
    "    sns.set(style=\"ticks\", font_scale=2.0)\n",
    "    ax.legend(loc=4)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "99c6d3eb7038ea1d21a6a465461e446f",
     "grade": false,
     "grade_id": "make_plot",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ax = plot_mean_abs_error(X_train, X_test, y_train, y_test, 150, check_random_state(0), t_best_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "34bb1ab95b5b714bcedb984b6cb4520e",
     "grade": true,
     "grade_id": "plot_assert",
     "locked": true,
     "points": 15,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(ax, mpl.axes.Axes, msg=\"Your function should return a matplotlib.axes.Axes object.\")\n",
    "\n",
    "assert_equal(len(ax.lines), 2)\n",
    "\n",
    "assert_is_not(len(ax.title.get_text()), 0, msg=\"Your plot doesn't have a title.\")\n",
    "assert_is_not(ax.xaxis.get_label_text(), '', msg=\"Change the x-axis label to something more descriptive.\")\n",
    "assert_is_not(ax.yaxis.get_label_text(), '', msg=\"Change the y-axis label to something more descriptive.\")\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "assert_equal(set(labels), set(['Train', 'Test']), msg=\"Make sure you have correct line labels.\")\n",
    "\n",
    "lines = ax.get_lines()\n",
    "train = [l for l in lines if l.get_label()=='Train'][0]\n",
    "test = [l for l in lines if l.get_label()=='Test'][0]\n",
    "\n",
    "train_xdata = train.get_xdata()\n",
    "train_ydata = train.get_ydata()\n",
    "\n",
    "test_xdata = test.get_xdata()\n",
    "test_ydata = test.get_ydata()\n",
    "\n",
    "assert_array_almost_equal(train_xdata[:5], [1, 2, 3, 4, 5])\n",
    "assert_array_almost_equal(train_ydata[:5], [0.03521127,  0.00704225,  0.00704225,  0.00704225,  0.00704225])\n",
    "\n",
    "assert_array_almost_equal(test_xdata[:5], [1, 2, 3, 4, 5])\n",
    "assert_array_almost_equal(test_ydata[:5], [0.08333333,  0.11111111,  0.11111111,  0.11111111,  0.11111111])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
